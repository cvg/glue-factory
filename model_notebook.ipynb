{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Playground for build & test JPLDD Model",
   "id": "dba3c8a9840cad76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:31:09.968228Z",
     "start_time": "2024-04-15T16:31:05.586189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import pixel_shuffle, softmax\n",
    "import torchvision\n",
    "from torchvision.models import resnet\n",
    "from torch.nn.modules.utils import _pair\n",
    "from typing import Optional, Callable"
   ],
   "id": "1d0af421261c138",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now Load sample Image:",
   "id": "d99538ad0f6de74c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Add Network Definitions\n",
    "ALIKED Backbone Encoder Parts"
   ],
   "id": "5dda71ea516e0bd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:31:22.346961Z",
     "start_time": "2024-04-15T16:31:22.314594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "aliked_cfgs = {\n",
    "    \"aliked-t16\": {\n",
    "        \"c1\": 8,\n",
    "        \"c2\": 16,\n",
    "        \"c3\": 32,\n",
    "        \"c4\": 64,\n",
    "        \"dim\": 64,\n",
    "        \"K\": 3,\n",
    "        \"M\": 16,\n",
    "    },\n",
    "    \"aliked-n16\": {\n",
    "        \"c1\": 16,\n",
    "        \"c2\": 32,\n",
    "        \"c3\": 64,\n",
    "        \"c4\": 128,\n",
    "        \"dim\": 128,\n",
    "        \"K\": 3,\n",
    "        \"M\": 16,\n",
    "    },\n",
    "    \"aliked-n16rot\": {\n",
    "        \"c1\": 16,\n",
    "        \"c2\": 32,\n",
    "        \"c3\": 64,\n",
    "        \"c4\": 128,\n",
    "        \"dim\": 128,\n",
    "        \"K\": 3,\n",
    "        \"M\": 16,\n",
    "    },\n",
    "    \"aliked-n32\": {\n",
    "        \"c1\": 16,\n",
    "        \"c2\": 32,\n",
    "        \"c3\": 64,\n",
    "        \"c4\": 128,\n",
    "        \"dim\": 128,\n",
    "        \"K\": 3,\n",
    "        \"M\": 32,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class AlikedEncoder(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super().__init__()\n",
    "        # get configurations\n",
    "        c1, c2, c3, c4, dim = conf[\"c1\"], conf[\"c2\"], conf[\"c3\"], conf[\"c4\"], conf[\"dim\"]\n",
    "        conv_types = [\"conv\", \"conv\", \"dcn\", \"dcn\"]\n",
    "        mask = False\n",
    "\n",
    "        # build model\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.pool4 = nn.AvgPool2d(kernel_size=4, stride=4)\n",
    "        self.norm = nn.BatchNorm2d\n",
    "        self.gate = nn.SELU(inplace=True)\n",
    "        self.block1 = ConvBlock(3, c1, self.gate, self.norm, conv_type=conv_types[0])\n",
    "        self.block2 = ResBlock(\n",
    "            c1,\n",
    "            c2,\n",
    "            1,\n",
    "            nn.Conv2d(c1, c2, 1),\n",
    "            gate=self.gate,\n",
    "            norm_layer=self.norm,\n",
    "            conv_type=conv_types[1],\n",
    "        )\n",
    "        self.block3 = ResBlock(\n",
    "            c2,\n",
    "            c3,\n",
    "            1,\n",
    "            nn.Conv2d(c2, c3, 1),\n",
    "            gate=self.gate,\n",
    "            norm_layer=self.norm,\n",
    "            conv_type=conv_types[2],\n",
    "            mask=mask,\n",
    "        )\n",
    "        self.block4 = ResBlock(\n",
    "            c3,\n",
    "            c4,\n",
    "            1,\n",
    "            nn.Conv2d(c3, c4, 1),\n",
    "            gate=self.gate,\n",
    "            norm_layer=self.norm,\n",
    "            conv_type=conv_types[3],\n",
    "            mask=mask,\n",
    "        )\n",
    "        self.conv1 = resnet.conv1x1(c1, dim // 4)\n",
    "        self.conv2 = resnet.conv1x1(c2, dim // 4)\n",
    "        self.conv3 = resnet.conv1x1(c3, dim // 4)\n",
    "        self.conv4 = resnet.conv1x1(dim, dim // 4)\n",
    "        self.upsample2 = nn.Upsample(\n",
    "            scale_factor=2, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "        self.upsample4 = nn.Upsample(\n",
    "            scale_factor=4, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "        self.upsample8 = nn.Upsample(\n",
    "            scale_factor=8, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "        self.upsample32 = nn.Upsample(\n",
    "            scale_factor=32, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        # ================================== feature encoder\n",
    "        x1 = self.block1(image)  # B x c1 x H x W\n",
    "        x2 = self.pool2(x1)\n",
    "        x2 = self.block2(x2)  # B x c2 x H/2 x W/2\n",
    "        x3 = self.pool4(x2)\n",
    "        x3 = self.block3(x3)  # B x c3 x H/8 x W/8\n",
    "        x4 = self.pool4(x3)\n",
    "        x4 = self.block4(x4)  # B x dim x H/32 x W/32\n",
    "        # ================================== feature aggregation\n",
    "        x1 = self.gate(self.conv1(x1))  # B x dim//4 x H x W\n",
    "        x2 = self.gate(self.conv2(x2))  # B x dim//4 x H//2 x W//2\n",
    "        x3 = self.gate(self.conv3(x3))  # B x dim//4 x H//8 x W//8\n",
    "        x4 = self.gate(self.conv4(x4))  # B x dim//4 x H//32 x W//32\n",
    "        x2_up = self.upsample2(x2)  # B x dim//4 x H x W\n",
    "        x3_up = self.upsample8(x3)  # B x dim//4 x H x W\n",
    "        x4_up = self.upsample32(x4)  # B x dim//4 x H x W\n",
    "        x1234 = torch.cat([x1, x2_up, x3_up, x4_up], dim=1)\n",
    "\n",
    "        return x1234\n",
    "    \n",
    "class DeformableConv2d(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "            mask=False,\n",
    "    ):\n",
    "        super(DeformableConv2d, self).__init__()\n",
    "\n",
    "        self.padding = padding\n",
    "        self.mask = mask\n",
    "\n",
    "        self.channel_num = (\n",
    "            3 * kernel_size * kernel_size if mask else 2 * kernel_size * kernel_size\n",
    "        )\n",
    "        self.offset_conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            self.channel_num,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=self.padding,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.regular_conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=self.padding,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "        max_offset = max(h, w) / 4.0\n",
    "\n",
    "        out = self.offset_conv(x)\n",
    "        if self.mask:\n",
    "            o1, o2, mask = torch.chunk(out, 3, dim=1)\n",
    "            offset = torch.cat((o1, o2), dim=1)\n",
    "            mask = torch.sigmoid(mask)\n",
    "        else:\n",
    "            offset = out\n",
    "            mask = None\n",
    "        offset = offset.clamp(-max_offset, max_offset)\n",
    "        x = torchvision.ops.deform_conv2d(\n",
    "            input=x,\n",
    "            offset=offset,\n",
    "            weight=self.regular_conv.weight,\n",
    "            bias=self.regular_conv.bias,\n",
    "            padding=self.padding,\n",
    "            mask=mask,\n",
    "        )\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_conv(\n",
    "        inplanes,\n",
    "        planes,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        bias=False,\n",
    "        conv_type=\"conv\",\n",
    "        mask=False,\n",
    "):\n",
    "    if conv_type == \"conv\":\n",
    "        conv = nn.Conv2d(\n",
    "            inplanes,\n",
    "            planes,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=bias,\n",
    "        )\n",
    "    elif conv_type == \"dcn\":\n",
    "        conv = DeformableConv2d(\n",
    "            inplanes,\n",
    "            planes,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=_pair(padding),\n",
    "            bias=bias,\n",
    "            mask=mask,\n",
    "        )\n",
    "    else:\n",
    "        raise TypeError\n",
    "    return conv\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            gate: Optional[Callable[..., nn.Module]] = None,\n",
    "            norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "            conv_type: str = \"conv\",\n",
    "            mask: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if gate is None:\n",
    "            self.gate = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            self.gate = gate\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self.conv1 = get_conv(\n",
    "            in_channels, out_channels, kernel_size=3, conv_type=conv_type, mask=mask\n",
    "        )\n",
    "        self.bn1 = norm_layer(out_channels)\n",
    "        self.conv2 = get_conv(\n",
    "            out_channels, out_channels, kernel_size=3, conv_type=conv_type, mask=mask\n",
    "        )\n",
    "        self.bn2 = norm_layer(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.gate(self.bn1(self.conv1(x)))  # B x in_channels x H x W\n",
    "        x = self.gate(self.bn2(self.conv2(x)))  # B x out_channels x H x W\n",
    "        return x\n",
    "\n",
    "\n",
    "# modified based on torchvision\\models\\resnet.py#27->BasicBlock\n",
    "class ResBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            inplanes: int,\n",
    "            planes: int,\n",
    "            stride: int = 1,\n",
    "            downsample: Optional[nn.Module] = None,\n",
    "            groups: int = 1,\n",
    "            base_width: int = 64,\n",
    "            dilation: int = 1,\n",
    "            gate: Optional[Callable[..., nn.Module]] = None,\n",
    "            norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "            conv_type: str = \"conv\",\n",
    "            mask: bool = False,\n",
    "    ) -> None:\n",
    "        super(ResBlock, self).__init__()\n",
    "        if gate is None:\n",
    "            self.gate = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            self.gate = gate\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"ResBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in ResBlock\")\n",
    "        # Both self.conv1 and self.downsample layers\n",
    "        # downsample the input when stride != 1\n",
    "        self.conv1 = get_conv(\n",
    "            inplanes, planes, kernel_size=3, conv_type=conv_type, mask=mask\n",
    "        )\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.conv2 = get_conv(\n",
    "            planes, planes, kernel_size=3, conv_type=conv_type, mask=mask\n",
    "        )\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.gate(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.gate(out)\n",
    "\n",
    "        return out"
   ],
   "id": "9dc0cf0724b92a8e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SDDH(Descriptors) also from ALIKED",
   "id": "da1f2cd3fe62285e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:31:29.662437Z",
     "start_time": "2024-04-15T16:31:29.645990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Helper used in SDDH\n",
    "def get_patches(\n",
    "        tensor: torch.Tensor, required_corners: torch.Tensor, ps: int\n",
    ") -> torch.Tensor:\n",
    "    c, h, w = tensor.shape\n",
    "    corner = (required_corners - ps / 2 + 1).long()\n",
    "    corner[:, 0] = corner[:, 0].clamp(min=0, max=w - 1 - ps)\n",
    "    corner[:, 1] = corner[:, 1].clamp(min=0, max=h - 1 - ps)\n",
    "    offset = torch.arange(0, ps)\n",
    "\n",
    "    kw = {\"indexing\": \"ij\"} if torch.__version__ >= \"1.10\" else {}\n",
    "    x, y = torch.meshgrid(offset, offset, **kw)\n",
    "    patches = torch.stack((x, y)).permute(2, 1, 0).unsqueeze(2)\n",
    "    patches = patches.to(corner) + corner[None, None]\n",
    "    pts = patches.reshape(-1, 2)\n",
    "    sampled = tensor.permute(1, 2, 0)[tuple(pts.T)[::-1]]\n",
    "    sampled = sampled.reshape(ps, ps, -1, c)\n",
    "    assert sampled.shape[:3] == patches.shape[:3]\n",
    "    return sampled.permute(2, 3, 0, 1)\n",
    "\n",
    "\n",
    "class SDDH(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dims: int,\n",
    "            kernel_size: int = 3,\n",
    "            n_pos: int = 8,\n",
    "            gate=nn.ReLU(),\n",
    "            conv2D=False,\n",
    "            mask=False,\n",
    "    ):\n",
    "        super(SDDH, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_pos = n_pos\n",
    "        self.conv2D = conv2D\n",
    "        self.mask = mask\n",
    "\n",
    "        self.get_patches_func = get_patches\n",
    "\n",
    "        # estimate offsets\n",
    "        self.channel_num = 3 * n_pos if mask else 2 * n_pos\n",
    "        self.offset_conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                dims,\n",
    "                self.channel_num,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=True,\n",
    "            ),\n",
    "            gate,\n",
    "            nn.Conv2d(\n",
    "                self.channel_num,\n",
    "                self.channel_num,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # sampled feature conv\n",
    "        self.sf_conv = nn.Conv2d(\n",
    "            dims, dims, kernel_size=1, stride=1, padding=0, bias=False\n",
    "        )\n",
    "\n",
    "        # convM\n",
    "        if not conv2D:\n",
    "            # deformable desc weights\n",
    "            agg_weights = torch.nn.Parameter(torch.rand(n_pos, dims, dims))\n",
    "            self.register_parameter(\"agg_weights\", agg_weights)\n",
    "        else:\n",
    "            self.convM = nn.Conv2d(\n",
    "                dims * n_pos, dims, kernel_size=1, stride=1, padding=0, bias=False\n",
    "            )\n",
    "\n",
    "    def forward(self, x, keypoints):\n",
    "        # x: [B,C,H,W]\n",
    "        # keypoints: list, [[N_kpts,2], ...] (w,h)\n",
    "        b, c, h, w = x.shape\n",
    "        wh = torch.tensor([[w - 1, h - 1]], device=x.device)\n",
    "        max_offset = max(h, w) / 4.0\n",
    "\n",
    "        offsets = []\n",
    "        descriptors = []\n",
    "        # get offsets for each keypoint\n",
    "        for ib in range(b):\n",
    "            xi, kptsi = x[ib], keypoints[ib]\n",
    "            kptsi_wh = (kptsi / 2 + 0.5) * wh\n",
    "            N_kpts = len(kptsi)\n",
    "\n",
    "            if self.kernel_size > 1:\n",
    "                patch = self.get_patches_func(\n",
    "                    xi, kptsi_wh.long(), self.kernel_size\n",
    "                )  # [N_kpts, C, K, K]\n",
    "            else:\n",
    "                kptsi_wh_long = kptsi_wh.long()\n",
    "                patch = (\n",
    "                    xi[:, kptsi_wh_long[:, 1], kptsi_wh_long[:, 0]]\n",
    "                    .permute(1, 0)\n",
    "                    .reshape(N_kpts, c, 1, 1)\n",
    "                )\n",
    "\n",
    "            offset = self.offset_conv(patch).clamp(\n",
    "                -max_offset, max_offset\n",
    "            )  # [N_kpts, 2*n_pos, 1, 1]\n",
    "            if self.mask:\n",
    "                offset = (\n",
    "                    offset[:, :, 0, 0].view(N_kpts, 3, self.n_pos).permute(0, 2, 1)\n",
    "                )  # [N_kpts, n_pos, 3]\n",
    "                offset = offset[:, :, :-1]  # [N_kpts, n_pos, 2]\n",
    "                mask_weight = torch.sigmoid(offset[:, :, -1])  # [N_kpts, n_pos]\n",
    "            else:\n",
    "                offset = (\n",
    "                    offset[:, :, 0, 0].view(N_kpts, 2, self.n_pos).permute(0, 2, 1)\n",
    "                )  # [N_kpts, n_pos, 2]\n",
    "            offsets.append(offset)  # for visualization\n",
    "\n",
    "            # get sample positions\n",
    "            pos = kptsi_wh.unsqueeze(1) + offset  # [N_kpts, n_pos, 2]\n",
    "            pos = 2.0 * pos / wh[None] - 1\n",
    "            pos = pos.reshape(1, N_kpts * self.n_pos, 1, 2)\n",
    "\n",
    "            # sample features\n",
    "            features = F.grid_sample(\n",
    "                xi.unsqueeze(0), pos, mode=\"bilinear\", align_corners=True\n",
    "            )  # [1,C,(N_kpts*n_pos),1]\n",
    "            features = features.reshape(c, N_kpts, self.n_pos, 1).permute(\n",
    "                1, 0, 2, 3\n",
    "            )  # [N_kpts, C, n_pos, 1]\n",
    "            if self.mask:\n",
    "                features = torch.einsum(\"ncpo,np->ncpo\", features, mask_weight)\n",
    "\n",
    "            features = torch.selu_(self.sf_conv(features)).squeeze(\n",
    "                -1\n",
    "            )  # [N_kpts, C, n_pos]\n",
    "            # convM\n",
    "            if not self.conv2D:\n",
    "                descs = torch.einsum(\n",
    "                    \"ncp,pcd->nd\", features, self.agg_weights\n",
    "                )  # [N_kpts, C]\n",
    "            else:\n",
    "                features = features.reshape(N_kpts, -1)[\n",
    "                           :, :, None, None\n",
    "                           ]  # [N_kpts, C*n_pos, 1, 1]\n",
    "                descs = self.convM(features).squeeze()  # [N_kpts, C]\n",
    "\n",
    "            # normalize\n",
    "            descs = F.normalize(descs, p=2.0, dim=1)\n",
    "            descriptors.append(descs)\n",
    "\n",
    "        return descriptors, offsets\n"
   ],
   "id": "eec3a2a1d6daea23",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SMH(Keypoint+Junction-Map) taken from ALIKED",
   "id": "67f157043d18d7c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:31:33.022932Z",
     "start_time": "2024-04-15T16:31:33.018026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SMH(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SMH, self).__init__()\n",
    "        self.gate = nn.SELU(inplace=True)\n",
    "        self.score_head = nn.Sequential(\n",
    "            resnet.conv1x1(input_dim, 8),\n",
    "            self.gate,\n",
    "            resnet.conv3x3(8, 4),\n",
    "            self.gate,\n",
    "            resnet.conv3x3(4, 4),\n",
    "            self.gate,\n",
    "            resnet.conv3x3(4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # expects feature map not normalized\n",
    "        return torch.sigmoid(self.score_head(x))"
   ],
   "id": "d4eeae3c4a374e7b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DKD(Extract Keypoints from heatmap)",
   "id": "203cee1db77f926d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:31:35.211519Z",
     "start_time": "2024-04-15T16:31:35.198132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Util Needed for DKD\n",
    "def simple_nms(scores: torch.Tensor, nms_radius: int):\n",
    "    \"\"\"Fast Non-maximum suppression to remove nearby points\"\"\"\n",
    "\n",
    "    zeros = torch.zeros_like(scores)\n",
    "    max_mask = scores == torch.nn.functional.max_pool2d(\n",
    "        scores, kernel_size=nms_radius * 2 + 1, stride=1, padding=nms_radius\n",
    "    )\n",
    "\n",
    "    for _ in range(2):\n",
    "        supp_mask = (\n",
    "                torch.nn.functional.max_pool2d(\n",
    "                    max_mask.float(),\n",
    "                    kernel_size=nms_radius * 2 + 1,\n",
    "                    stride=1,\n",
    "                    padding=nms_radius,\n",
    "                )\n",
    "                > 0\n",
    "        )\n",
    "        supp_scores = torch.where(supp_mask, zeros, scores)\n",
    "        new_max_mask = supp_scores == torch.nn.functional.max_pool2d(\n",
    "            supp_scores, kernel_size=nms_radius * 2 + 1, stride=1, padding=nms_radius\n",
    "        )\n",
    "        max_mask = max_mask | (new_max_mask & (~supp_mask))\n",
    "    return torch.where(max_mask, scores, zeros)\n",
    "\n",
    "class DKD(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            radius: int = 2,\n",
    "            top_k: int = 0,\n",
    "            scores_th: float = 0.2,\n",
    "            n_limit: int = 20000,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            radius: soft detection radius, kernel size is (2 * radius + 1)\n",
    "            top_k: top_k > 0: return top k keypoints\n",
    "            scores_th: top_k <= 0 threshold mode:\n",
    "                scores_th > 0: return keypoints with scores>scores_th\n",
    "                else: return keypoints with scores > scores.mean()\n",
    "            n_limit: max number of keypoint in threshold mode\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.radius = radius\n",
    "        self.top_k = top_k\n",
    "        self.scores_th = scores_th\n",
    "        self.n_limit = n_limit\n",
    "        self.kernel_size = 2 * self.radius + 1\n",
    "        self.temperature = 0.1  # tuned temperature\n",
    "        self.unfold = nn.Unfold(kernel_size=self.kernel_size, padding=self.radius)\n",
    "        # local xy grid\n",
    "        x = torch.linspace(-self.radius, self.radius, self.kernel_size)\n",
    "        # (kernel_size*kernel_size) x 2 : (w,h)\n",
    "        kw = {\"indexing\": \"ij\"} if torch.__version__ >= \"1.10\" else {}\n",
    "        self.hw_grid = (\n",
    "            torch.stack(torch.meshgrid([x, x], **kw)).view(2, -1).t()[:, [1, 0]]\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            scores_map: torch.Tensor,\n",
    "            sub_pixel: bool = True,\n",
    "            image_size: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param scores_map: Bx1xHxW\n",
    "        :param descriptor_map: BxCxHxW\n",
    "        :param sub_pixel: whether to use sub-pixel keypoint detection\n",
    "        :return: kpts: list[Nx2,...]; kptscores: list[N,....] normalised position: -1~1\n",
    "        \"\"\"\n",
    "        b, c, h, w = scores_map.shape\n",
    "        scores_nograd = scores_map.detach()\n",
    "        nms_scores = simple_nms(scores_nograd, self.radius)\n",
    "\n",
    "        # remove border\n",
    "        nms_scores[:, :, : self.radius, :] = 0\n",
    "        nms_scores[:, :, :, : self.radius] = 0\n",
    "        if image_size is not None:\n",
    "            for i in range(scores_map.shape[0]):\n",
    "                w, h = image_size[i].long()\n",
    "                nms_scores[i, :, h.item() - self.radius:, :] = 0\n",
    "                nms_scores[i, :, :, w.item() - self.radius:] = 0\n",
    "        else:\n",
    "            nms_scores[:, :, -self.radius:, :] = 0\n",
    "            nms_scores[:, :, :, -self.radius:] = 0\n",
    "\n",
    "        # detect keypoints without grad\n",
    "        if self.top_k > 0:\n",
    "            topk = torch.topk(nms_scores.view(b, -1), self.top_k)\n",
    "            indices_keypoints = [topk.indices[i] for i in range(b)]  # B x top_k\n",
    "        else:\n",
    "            if self.scores_th > 0:\n",
    "                masks = nms_scores > self.scores_th\n",
    "                if masks.sum() == 0:\n",
    "                    th = scores_nograd.reshape(b, -1).mean(dim=1)  # th = self.scores_th\n",
    "                    masks = nms_scores > th.reshape(b, 1, 1, 1)\n",
    "            else:\n",
    "                th = scores_nograd.reshape(b, -1).mean(dim=1)  # th = self.scores_th\n",
    "                masks = nms_scores > th.reshape(b, 1, 1, 1)\n",
    "            masks = masks.reshape(b, -1)\n",
    "\n",
    "            indices_keypoints = []  # list, B x (any size)\n",
    "            scores_view = scores_nograd.reshape(b, -1)\n",
    "            for mask, scores in zip(masks, scores_view):\n",
    "                indices = mask.nonzero()[:, 0]\n",
    "                if len(indices) > self.n_limit:\n",
    "                    kpts_sc = scores[indices]\n",
    "                    sort_idx = kpts_sc.sort(descending=True)[1]\n",
    "                    sel_idx = sort_idx[: self.n_limit]\n",
    "                    indices = indices[sel_idx]\n",
    "                indices_keypoints.append(indices)\n",
    "\n",
    "        wh = torch.tensor([w - 1, h - 1], device=scores_nograd.device)\n",
    "\n",
    "        keypoints = []\n",
    "        scoredispersitys = []\n",
    "        kptscores = []\n",
    "        if sub_pixel:\n",
    "            # detect soft keypoints with grad backpropagation\n",
    "            patches = self.unfold(scores_map)  # B x (kernel**2) x (H*W)\n",
    "            self.hw_grid = self.hw_grid.to(scores_map)  # to device\n",
    "            for b_idx in range(b):\n",
    "                patch = patches[b_idx].t()  # (H*W) x (kernel**2)\n",
    "                indices_kpt = indices_keypoints[\n",
    "                    b_idx\n",
    "                ]  # one dimension vector, say its size is M\n",
    "                patch_scores = patch[indices_kpt]  # M x (kernel**2)\n",
    "                keypoints_xy_nms = torch.stack(\n",
    "                    [indices_kpt % w, torch.div(indices_kpt, w, rounding_mode=\"trunc\")],\n",
    "                    dim=1,\n",
    "                )  # Mx2\n",
    "\n",
    "                # max is detached to prevent undesired backprop loops in the graph\n",
    "                max_v = patch_scores.max(dim=1).values.detach()[:, None]\n",
    "                x_exp = (\n",
    "                        (patch_scores - max_v) / self.temperature\n",
    "                ).exp()  # M * (kernel**2), in [0, 1]\n",
    "\n",
    "                # \\frac{ \\sum{(i,j) \\times \\exp(x/T)} }{ \\sum{\\exp(x/T)} }\n",
    "                xy_residual = (\n",
    "                        x_exp @ self.hw_grid / x_exp.sum(dim=1)[:, None]\n",
    "                )  # Soft-argmax, Mx2\n",
    "\n",
    "                hw_grid_dist2 = (\n",
    "                        torch.norm(\n",
    "                            (self.hw_grid[None, :, :] - xy_residual[:, None, :])\n",
    "                            / self.radius,\n",
    "                            dim=-1,\n",
    "                        )\n",
    "                        ** 2\n",
    "                )\n",
    "                scoredispersity = (x_exp * hw_grid_dist2).sum(dim=1) / x_exp.sum(dim=1)\n",
    "\n",
    "                # compute result keypoints\n",
    "                keypoints_xy = keypoints_xy_nms + xy_residual\n",
    "                keypoints_xy = keypoints_xy / wh * 2 - 1  # (w,h) -> (-1~1,-1~1)\n",
    "\n",
    "                kptscore = torch.nn.functional.grid_sample(\n",
    "                    scores_map[b_idx].unsqueeze(0),\n",
    "                    keypoints_xy.view(1, 1, -1, 2),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=True,\n",
    "                )[\n",
    "                           0, 0, 0, :\n",
    "                           ]  # CxN\n",
    "\n",
    "                keypoints.append(keypoints_xy)\n",
    "                scoredispersitys.append(scoredispersity)\n",
    "                kptscores.append(kptscore)\n",
    "        else:\n",
    "            for b_idx in range(b):\n",
    "                indices_kpt = indices_keypoints[\n",
    "                    b_idx\n",
    "                ]  # one dimension vector, say its size is M\n",
    "                # To avoid warning: UserWarning: __floordiv__ is deprecated\n",
    "                keypoints_xy_nms = torch.stack(\n",
    "                    [indices_kpt % w, torch.div(indices_kpt, w, rounding_mode=\"trunc\")],\n",
    "                    dim=1,\n",
    "                )  # Mx2\n",
    "                keypoints_xy = keypoints_xy_nms / wh * 2 - 1  # (w,h) -> (-1~1,-1~1)\n",
    "                kptscore = torch.nn.functional.grid_sample(\n",
    "                    scores_map[b_idx].unsqueeze(0),\n",
    "                    keypoints_xy.view(1, 1, -1, 2),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=True,\n",
    "                )[\n",
    "                           0, 0, 0, :\n",
    "                           ]  # CxN\n",
    "                keypoints.append(keypoints_xy)\n",
    "                scoredispersitys.append(kptscore)  # for jit.script compatability\n",
    "                kptscores.append(kptscore)\n",
    "\n",
    "        return keypoints, scoredispersitys, kptscores"
   ],
   "id": "a9c71b621121c0e3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Line Heatmap decoder (Taken from SOLD2)",
   "id": "6f265151b4d01d4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:31:39.661828Z",
     "start_time": "2024-04-15T16:31:39.654775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PixelShuffleDecoder(nn.Module):\n",
    "    \"\"\" Pixel shuffle decoder. \"\"\"\n",
    "\n",
    "    def __init__(self, input_feat_dim=128, num_upsample=2, output_channel=2):\n",
    "        super(PixelShuffleDecoder, self).__init__()\n",
    "        # Get channel parameters\n",
    "        self.channel_conf = self.get_channel_conf(num_upsample)\n",
    "\n",
    "        # Define the pixel shuffle\n",
    "        self.pixshuffle = nn.PixelShuffle(2)\n",
    "\n",
    "        # Process the feature\n",
    "        self.conv_block_lst = []\n",
    "        # The input block\n",
    "        self.conv_block_lst.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(input_feat_dim, self.channel_conf[0],\n",
    "                          kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(self.channel_conf[0]),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "\n",
    "        # Intermediate block\n",
    "        for channel in self.channel_conf[1:-1]:\n",
    "            self.conv_block_lst.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(channel, channel, kernel_size=3,\n",
    "                              stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(channel),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                ))\n",
    "\n",
    "        # Output block\n",
    "        self.conv_block_lst.append(\n",
    "            nn.Conv2d(self.channel_conf[-1], output_channel,\n",
    "                      kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "        self.conv_block_lst = nn.ModuleList(self.conv_block_lst)\n",
    "\n",
    "    # Get num of channels based on number of upsampling.\n",
    "    def get_channel_conf(self, num_upsample):\n",
    "        if num_upsample == 2:\n",
    "            return [256, 64, 16]\n",
    "        elif num_upsample == 3:\n",
    "            return [256, 64, 16, 4]\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        # Iterate til output block\n",
    "        out = input_features\n",
    "        for block in self.conv_block_lst[:-1]:\n",
    "            out = block(out)\n",
    "            out = self.pixshuffle(out)\n",
    "\n",
    "        # Output layer\n",
    "        out = self.conv_block_lst[-1](out)\n",
    "\n",
    "        return out"
   ],
   "id": "af6a2f7f335b4131",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Add Line Extractor (taken from SOLD2)",
   "id": "556926434ac04458"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:31:42.039850Z",
     "start_time": "2024-04-15T16:31:41.987423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def line_map_to_segments(junctions, line_map):\n",
    "    \"\"\" Convert a line map to a Nx2x2 list of segments. \"\"\"\n",
    "    line_map_tmp = line_map.copy()\n",
    "\n",
    "    output_segments = np.zeros([0, 2, 2])\n",
    "    for idx in range(junctions.shape[0]):\n",
    "        # if no connectivity, just skip it\n",
    "        if line_map_tmp[idx, :].sum() == 0:\n",
    "            continue\n",
    "        # Record the line segment\n",
    "        else:\n",
    "            for idx2 in np.where(line_map_tmp[idx, :] == 1)[0]:\n",
    "                p1 = junctions[idx, :]  # HW format\n",
    "                p2 = junctions[idx2, :]\n",
    "                single_seg = np.concatenate([p1[None, ...], p2[None, ...]],\n",
    "                                            axis=0)\n",
    "                output_segments = np.concatenate(\n",
    "                    (output_segments, single_seg[None, ...]), axis=0)\n",
    "\n",
    "                # Update line_map\n",
    "                line_map_tmp[idx, idx2] = 0\n",
    "                line_map_tmp[idx2, idx] = 0\n",
    "\n",
    "    return output_segments\n",
    "\n",
    "\n",
    "# Taken from SOLD2\n",
    "def convert_junc_predictions(predictions, grid_size,\n",
    "                             detect_thresh=1 / 65, topk=300):\n",
    "    \"\"\" Convert torch predictions to numpy arrays for evaluation. \"\"\"\n",
    "    # Convert to probability outputs first\n",
    "    junc_prob = softmax(predictions.detach(), dim=1).cpu()\n",
    "    junc_pred = junc_prob[:, :-1, :, :]\n",
    "\n",
    "    junc_prob_np = junc_prob.numpy().transpose(0, 2, 3, 1)[:, :, :, :-1]\n",
    "    junc_prob_np = np.sum(junc_prob_np, axis=-1)\n",
    "    junc_pred_np = pixel_shuffle(\n",
    "        junc_pred, grid_size).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    junc_pred_np_nms = super_nms(junc_pred_np, grid_size, detect_thresh, topk)\n",
    "    junc_pred_np = junc_pred_np.squeeze(-1)\n",
    "\n",
    "    return {\"junc_pred\": junc_pred_np, \"junc_pred_nms\": junc_pred_np_nms,\n",
    "            \"junc_prob\": junc_prob_np}\n",
    "\n",
    "\n",
    "# Taken from SOLD2\n",
    "def super_nms(prob_predictions, dist_thresh, prob_thresh=0.01, top_k=0):\n",
    "    \"\"\" Non-maximum suppression adapted from SuperPoint. \"\"\"\n",
    "    # Iterate through batch dimension\n",
    "    im_h = prob_predictions.shape[1]\n",
    "    im_w = prob_predictions.shape[2]\n",
    "    output_lst = []\n",
    "    for i in range(prob_predictions.shape[0]):\n",
    "        # print(i)\n",
    "        prob_pred = prob_predictions[i, ...]\n",
    "        # Filter the points using prob_thresh\n",
    "        coord = np.where(prob_pred >= prob_thresh)  # HW format\n",
    "        points = np.concatenate((coord[0][..., None], coord[1][..., None]),\n",
    "                                axis=1)  # HW format\n",
    "\n",
    "        # Get the probability score\n",
    "        prob_score = prob_pred[points[:, 0], points[:, 1]]\n",
    "\n",
    "        # Perform super nms\n",
    "        # Modify the in_points to xy format (instead of HW format)\n",
    "        in_points = np.concatenate((coord[1][..., None], coord[0][..., None],\n",
    "                                    prob_score), axis=1).T\n",
    "        keep_points_, keep_inds = nms_fast(in_points, im_h, im_w, dist_thresh)\n",
    "        # Remember to flip outputs back to HW format\n",
    "        keep_points = np.round(np.flip(keep_points_[:2, :], axis=0).T)\n",
    "        keep_score = keep_points_[-1, :].T\n",
    "\n",
    "        # Whether we only keep the topk value\n",
    "        if (top_k > 0) or (top_k is None):\n",
    "            k = min([keep_points.shape[0], top_k])\n",
    "            keep_points = keep_points[:k, :]\n",
    "            keep_score = keep_score[:k]\n",
    "\n",
    "        # Re-compose the probability map\n",
    "        output_map = np.zeros([im_h, im_w])\n",
    "        output_map[keep_points[:, 0].astype(np.int),\n",
    "        keep_points[:, 1].astype(np.int)] = keep_score.squeeze()\n",
    "\n",
    "        output_lst.append(output_map[None, ...])\n",
    "\n",
    "    return np.concatenate(output_lst, axis=0)\n",
    "\n",
    "\n",
    "# Taken from SOLD2\n",
    "def nms_fast(in_corners, H, W, dist_thresh):\n",
    "    \"\"\"\n",
    "    Run a faster approximate Non-Max-Suppression on numpy corners shaped:\n",
    "      3xN [x_i,y_i,conf_i]^T\n",
    "\n",
    "    Algo summary: Create a grid sized HxW. Assign each corner location a 1,\n",
    "    rest are zeros. Iterate through all the 1's and convert them to -1 or 0.\n",
    "    Suppress points by setting nearby values to 0.\n",
    "\n",
    "    Grid Value Legend:\n",
    "    -1 : Kept.\n",
    "     0 : Empty or suppressed.\n",
    "     1 : To be processed (converted to either kept or supressed).\n",
    "\n",
    "    NOTE: The NMS first rounds points to integers, so NMS distance might not\n",
    "    be exactly dist_thresh. It also assumes points are within image boundary.\n",
    "\n",
    "    Inputs\n",
    "      in_corners - 3xN numpy array with corners [x_i, y_i, confidence_i]^T.\n",
    "      H - Image height.\n",
    "      W - Image width.\n",
    "      dist_thresh - Distance to suppress, measured as an infinite distance.\n",
    "    Returns\n",
    "      nmsed_corners - 3xN numpy matrix with surviving corners.\n",
    "      nmsed_inds - N length numpy vector with surviving corner indices.\n",
    "    \"\"\"\n",
    "    grid = np.zeros((H, W)).astype(int)  # Track NMS data.\n",
    "    inds = np.zeros((H, W)).astype(int)  # Store indices of points.\n",
    "    # Sort by confidence and round to nearest int.\n",
    "    inds1 = np.argsort(-in_corners[2, :])\n",
    "    corners = in_corners[:, inds1]\n",
    "    rcorners = corners[:2, :].round().astype(int)  # Rounded corners.\n",
    "    # Check for edge case of 0 or 1 corners.\n",
    "    if rcorners.shape[1] == 0:\n",
    "        return np.zeros((3, 0)).astype(int), np.zeros(0).astype(int)\n",
    "    if rcorners.shape[1] == 1:\n",
    "        out = np.vstack((rcorners, in_corners[2])).reshape(3, 1)\n",
    "        return out, np.zeros((1)).astype(int)\n",
    "    # Initialize the grid.\n",
    "    for i, rc in enumerate(rcorners.T):\n",
    "        grid[rcorners[1, i], rcorners[0, i]] = 1\n",
    "        inds[rcorners[1, i], rcorners[0, i]] = i\n",
    "    # Pad the border of the grid, so that we can NMS points near the border.\n",
    "    pad = dist_thresh\n",
    "    grid = np.pad(grid, ((pad, pad), (pad, pad)), mode='constant')\n",
    "    # Iterate through points, highest to lowest conf, suppress neighborhood.\n",
    "    count = 0\n",
    "    for i, rc in enumerate(rcorners.T):\n",
    "        # Account for top and left padding.\n",
    "        pt = (rc[0] + pad, rc[1] + pad)\n",
    "        if grid[pt[1], pt[0]] == 1:  # If not yet suppressed.\n",
    "            grid[pt[1] - pad:pt[1] + pad + 1, pt[0] - pad:pt[0] + pad + 1] = 0\n",
    "            grid[pt[1], pt[0]] = -1\n",
    "            count += 1\n",
    "    # Get all surviving -1's and return sorted array of remaining corners.\n",
    "    keepy, keepx = np.where(grid == -1)\n",
    "    keepy, keepx = keepy - pad, keepx - pad\n",
    "    inds_keep = inds[keepy, keepx]\n",
    "    out = corners[:, inds_keep]\n",
    "    values = out[-1, :]\n",
    "    inds2 = np.argsort(-values)\n",
    "    out = out[:, inds2]\n",
    "    out_inds = inds1[inds_keep[inds2]]\n",
    "    return out, out_inds\n",
    "\n",
    "class LineExtractor(object):\n",
    "    \"\"\"\n",
    "    Not learned method for line detection from junctions and line-heatmaps\n",
    "    Adapted from SOLD2 implementation\n",
    "    \"\"\"\n",
    "    # Line detector cfg in SOLD 2 can be found sold2/config/export_line_features.yaml under (line_detector_cfg)\n",
    "    # ToDo: check config handling & different configs for training and evaluation? (sold2 gives only detect thresh in training, other params not defined)\n",
    "    def __init__(self, device, line_extractor_cfg):\n",
    "        self.grid_size = line_extractor_cfg[\"grid_size\"]\n",
    "        self.junc_detect_thresh = line_extractor_cfg[\"junc_detect_thresh\"]\n",
    "        self.max_num_junctions = line_extractor_cfg[\"max_num_junctions\"]\n",
    "        self.device = device\n",
    "\n",
    "        self.detect_thresh = line_extractor_cfg[\"detect_thresh\"] # in cfg file: detection_thresh: 0.0153846 # 1/65\n",
    "        self.line_detector = LineSegmentDetectionModule(self.detect_thresh)\n",
    "\n",
    "    def __call__(self, junctions, line_heatmap, valid_mask=None):\n",
    "        junc_np = convert_junc_predictions(\n",
    "            junctions, self.grid_size,\n",
    "            self.junc_detect_thresh, self.max_num_junctions)\n",
    "        if valid_mask is None:\n",
    "            junctions = np.where(junc_np[\"junc_pred_nms\"].squeeze())\n",
    "        else:\n",
    "            junctions = np.where(junc_np[\"junc_pred_nms\"].squeeze()\n",
    "                                 * valid_mask)\n",
    "        junctions = np.concatenate(\n",
    "            [junctions[0][..., None], junctions[1][..., None]], axis=-1)\n",
    "\n",
    "        if line_heatmap.shape[1] == 2:\n",
    "            # Convert to single channel directly from here\n",
    "            heatmap = softmax(line_heatmap, dim=1)[:, 1:, :, :]\n",
    "        else:\n",
    "            heatmap = torch.sigmoid(line_heatmap)\n",
    "        heatmap = heatmap.cpu().numpy().transpose(0, 2, 3, 1)[0, :, :, 0]\n",
    "\n",
    "        # Run the line detector.\n",
    "        line_map, junctions, heatmap = self.line_detector.detect(\n",
    "            junctions, heatmap, device=self.device)\n",
    "        heatmap = heatmap.cpu().numpy()\n",
    "        if isinstance(line_map, torch.Tensor):\n",
    "            line_map = line_map.cpu().numpy()\n",
    "        if isinstance(junctions, torch.Tensor):\n",
    "            junctions = junctions.cpu().numpy()\n",
    "        line_segments = line_map_to_segments(junctions, line_map)\n",
    "\n",
    "        return line_segments, line_heatmap, junctions\n",
    "\n",
    "\n",
    "class LineSegmentDetectionModule(object):\n",
    "    \"\"\" Module extracting line segments from junctions and line heatmaps. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, detect_thresh, num_samples=64, sampling_method=\"local_max\",\n",
    "            inlier_thresh=0., heatmap_low_thresh=0.15, heatmap_high_thresh=0.2,\n",
    "            max_local_patch_radius=3, lambda_radius=2.,\n",
    "            use_candidate_suppression=False, nms_dist_tolerance=3.,\n",
    "            use_heatmap_refinement=False, heatmap_refine_cfg=None,\n",
    "            use_junction_refinement=False, junction_refine_cfg=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            detect_thresh: The probability threshold for mean activation (0. ~ 1.)\n",
    "            num_samples: Number of sampling locations along the line segments.\n",
    "            sampling_method: Sampling method on locations (\"bilinear\" or \"local_max\").\n",
    "            inlier_thresh: The min inlier ratio to satisfy (0. ~ 1.) => 0. means no threshold.\n",
    "            heatmap_low_thresh: The lowest threshold for the pixel to be considered as candidate in junction recovery.\n",
    "            heatmap_high_thresh: The higher threshold for NMS in junction recovery.\n",
    "            max_local_patch_radius: The max patch to be considered in local maximum search.\n",
    "            lambda_radius: The lambda factor in linear local maximum search formulation\n",
    "            use_candidate_suppression: Apply candidate suppression to break long segments into short sub-segments.\n",
    "            nms_dist_tolerance: The distance tolerance for nms. Decide whether the junctions are on the line.\n",
    "            use_heatmap_refinement: Use heatmap refinement method or not.\n",
    "            heatmap_refine_cfg: The configs for heatmap refinement methods.\n",
    "            use_junction_refinement: Use junction refinement method or not.\n",
    "            junction_refine_cfg: The configs for junction refinement methods.\n",
    "        \"\"\"\n",
    "        # Line detection parameters\n",
    "        self.detect_thresh = detect_thresh\n",
    "\n",
    "        # Line sampling parameters\n",
    "        self.num_samples = num_samples\n",
    "        self.sampling_method = sampling_method\n",
    "        self.inlier_thresh = inlier_thresh\n",
    "        self.local_patch_radius = max_local_patch_radius\n",
    "        self.lambda_radius = lambda_radius\n",
    "\n",
    "        # Detecting junctions on the boundary parameters\n",
    "        self.low_thresh = heatmap_low_thresh\n",
    "        self.high_thresh = heatmap_high_thresh\n",
    "\n",
    "        # Pre-compute the linspace sampler\n",
    "        self.sampler = np.linspace(0, 1, self.num_samples)\n",
    "        self.torch_sampler = torch.linspace(0, 1, self.num_samples)\n",
    "\n",
    "        # Long line segment suppression configuration\n",
    "        self.use_candidate_suppression = use_candidate_suppression\n",
    "        self.nms_dist_tolerance = nms_dist_tolerance\n",
    "\n",
    "        # Heatmap refinement configuration\n",
    "        self.use_heatmap_refinement = use_heatmap_refinement\n",
    "        self.heatmap_refine_cfg = heatmap_refine_cfg\n",
    "        if self.use_heatmap_refinement and self.heatmap_refine_cfg is None:\n",
    "            raise ValueError(\"[Error] Missing heatmap refinement config.\")\n",
    "\n",
    "        # Junction refinement configuration\n",
    "        self.use_junction_refinement = use_junction_refinement\n",
    "        self.junction_refine_cfg = junction_refine_cfg\n",
    "        if self.use_junction_refinement and self.junction_refine_cfg is None:\n",
    "            raise ValueError(\"[Error] Missing junction refinement config.\")\n",
    "\n",
    "    def convert_inputs(self, inputs, device):\n",
    "        \"\"\" Convert inputs to desired torch tensor. \"\"\"\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            outputs = torch.tensor(inputs, dtype=torch.float32, device=device)\n",
    "        elif isinstance(inputs, torch.Tensor):\n",
    "            outputs = inputs.to(torch.float32).to(device)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"[Error] Inputs must either be torch tensor or numpy ndarray.\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def detect(self, junctions, heatmap, device=torch.device(\"cpu\")):\n",
    "        \"\"\" Main function performing line segment detection. \"\"\"\n",
    "        # Convert inputs to torch tensor\n",
    "        junctions = self.convert_inputs(junctions, device=device)\n",
    "        heatmap = self.convert_inputs(heatmap, device=device)\n",
    "\n",
    "        # Perform the heatmap refinement\n",
    "        if self.use_heatmap_refinement:\n",
    "            if self.heatmap_refine_cfg[\"mode\"] == \"global\":\n",
    "                heatmap = self.refine_heatmap(\n",
    "                    heatmap,\n",
    "                    self.heatmap_refine_cfg[\"ratio\"],\n",
    "                    self.heatmap_refine_cfg[\"valid_thresh\"]\n",
    "                )\n",
    "            elif self.heatmap_refine_cfg[\"mode\"] == \"local\":\n",
    "                heatmap = self.refine_heatmap_local(\n",
    "                    heatmap,\n",
    "                    self.heatmap_refine_cfg[\"num_blocks\"],\n",
    "                    self.heatmap_refine_cfg[\"overlap_ratio\"],\n",
    "                    self.heatmap_refine_cfg[\"ratio\"],\n",
    "                    self.heatmap_refine_cfg[\"valid_thresh\"]\n",
    "                )\n",
    "\n",
    "        # Initialize empty line map\n",
    "        num_junctions = junctions.shape[0]\n",
    "        line_map_pred = torch.zeros([num_junctions, num_junctions],\n",
    "                                    device=device, dtype=torch.int32)\n",
    "\n",
    "        # Stop if there are not enough junctions\n",
    "        if num_junctions < 2:\n",
    "            return line_map_pred, junctions, heatmap\n",
    "\n",
    "        # Generate the candidate map\n",
    "        candidate_map = torch.triu(torch.ones(\n",
    "            [num_junctions, num_junctions], device=device, dtype=torch.int32),\n",
    "            diagonal=1)\n",
    "\n",
    "        # Fetch the image boundary\n",
    "        if len(heatmap.shape) > 2:\n",
    "            H, W, _ = heatmap.shape\n",
    "        else:\n",
    "            H, W = heatmap.shape\n",
    "\n",
    "        # Optionally perform candidate filtering\n",
    "        if self.use_candidate_suppression:\n",
    "            candidate_map = self.candidate_suppression(junctions,\n",
    "                                                       candidate_map)\n",
    "\n",
    "        # Fetch the candidates\n",
    "        candidate_index_map = torch.where(candidate_map)\n",
    "        candidate_index_map = torch.cat([candidate_index_map[0][..., None],\n",
    "                                         candidate_index_map[1][..., None]],\n",
    "                                        dim=-1)\n",
    "\n",
    "        # Get the corresponding start and end junctions\n",
    "        candidate_junc_start = junctions[candidate_index_map[:, 0], :]\n",
    "        candidate_junc_end = junctions[candidate_index_map[:, 1], :]\n",
    "\n",
    "        # Get the sampling locations (N x 64)\n",
    "        sampler = self.torch_sampler.to(device)[None, ...]\n",
    "        cand_samples_h = candidate_junc_start[:, 0:1] * sampler + \\\n",
    "                         candidate_junc_end[:, 0:1] * (1 - sampler)\n",
    "        cand_samples_w = candidate_junc_start[:, 1:2] * sampler + \\\n",
    "                         candidate_junc_end[:, 1:2] * (1 - sampler)\n",
    "\n",
    "        # Clip to image boundary\n",
    "        cand_h = torch.clamp(cand_samples_h, min=0, max=H - 1)\n",
    "        cand_w = torch.clamp(cand_samples_w, min=0, max=W - 1)\n",
    "\n",
    "        # Local maximum search\n",
    "        if self.sampling_method == \"local_max\":\n",
    "            # Compute normalized segment lengths\n",
    "            segments_length = torch.sqrt(torch.sum(\n",
    "                (candidate_junc_start.to(torch.float32) -\n",
    "                 candidate_junc_end.to(torch.float32)) ** 2, dim=-1))\n",
    "            normalized_seg_length = (segments_length\n",
    "                                     / (((H ** 2) + (W ** 2)) ** 0.5))\n",
    "\n",
    "            # Perform local max search\n",
    "            num_cand = cand_h.shape[0]\n",
    "            group_size = 10000\n",
    "            if num_cand > group_size:\n",
    "                num_iter = math.ceil(num_cand / group_size)\n",
    "                sampled_feat_lst = []\n",
    "                for iter_idx in range(num_iter):\n",
    "                    if not iter_idx == num_iter - 1:\n",
    "                        cand_h_ = cand_h[iter_idx * group_size:\n",
    "                                         (iter_idx + 1) * group_size, :]\n",
    "                        cand_w_ = cand_w[iter_idx * group_size:\n",
    "                                         (iter_idx + 1) * group_size, :]\n",
    "                        normalized_seg_length_ = normalized_seg_length[\n",
    "                                                 iter_idx * group_size: (iter_idx + 1) * group_size]\n",
    "                    else:\n",
    "                        cand_h_ = cand_h[iter_idx * group_size:, :]\n",
    "                        cand_w_ = cand_w[iter_idx * group_size:, :]\n",
    "                        normalized_seg_length_ = normalized_seg_length[\n",
    "                                                 iter_idx * group_size:]\n",
    "                    sampled_feat_ = self.detect_local_max(\n",
    "                        heatmap, cand_h_, cand_w_, H, W,\n",
    "                        normalized_seg_length_, device)\n",
    "                    sampled_feat_lst.append(sampled_feat_)\n",
    "                sampled_feat = torch.cat(sampled_feat_lst, dim=0)\n",
    "            else:\n",
    "                sampled_feat = self.detect_local_max(\n",
    "                    heatmap, cand_h, cand_w, H, W,\n",
    "                    normalized_seg_length, device)\n",
    "        # Bilinear sampling\n",
    "        elif self.sampling_method == \"bilinear\":\n",
    "            # Perform bilinear sampling\n",
    "            sampled_feat = self.detect_bilinear(\n",
    "                heatmap, cand_h, cand_w, H, W, device)\n",
    "        else:\n",
    "            raise ValueError(\"[Error] Unknown sampling method.\")\n",
    "\n",
    "        # [Simple threshold detection]\n",
    "        # detection_results is a mask over all candidates\n",
    "        detection_results = (torch.mean(sampled_feat, dim=-1)\n",
    "                             > self.detect_thresh)\n",
    "\n",
    "        # [Inlier threshold detection]\n",
    "        if self.inlier_thresh > 0.:\n",
    "            inlier_ratio = torch.sum(\n",
    "                sampled_feat > self.detect_thresh,\n",
    "                dim=-1).to(torch.float32) / self.num_samples\n",
    "            detection_results_inlier = inlier_ratio >= self.inlier_thresh\n",
    "            detection_results = detection_results * detection_results_inlier\n",
    "\n",
    "        # Convert detection results back to line_map_pred\n",
    "        detected_junc_indexes = candidate_index_map[detection_results, :]\n",
    "        line_map_pred[detected_junc_indexes[:, 0],\n",
    "        detected_junc_indexes[:, 1]] = 1\n",
    "        line_map_pred[detected_junc_indexes[:, 1],\n",
    "        detected_junc_indexes[:, 0]] = 1\n",
    "\n",
    "        # Perform junction refinement\n",
    "        if self.use_junction_refinement and len(detected_junc_indexes) > 0:\n",
    "            junctions, line_map_pred = self.refine_junction_perturb(\n",
    "                junctions, line_map_pred, heatmap, H, W, device)\n",
    "\n",
    "        return line_map_pred, junctions, heatmap\n",
    "\n",
    "    def refine_heatmap(self, heatmap, ratio=0.2, valid_thresh=1e-2):\n",
    "        \"\"\" Global heatmap refinement method. \"\"\"\n",
    "        # Grab the top 10% values\n",
    "        heatmap_values = heatmap[heatmap > valid_thresh]\n",
    "        sorted_values = torch.sort(heatmap_values, descending=True)[0]\n",
    "        top10_len = math.ceil(sorted_values.shape[0] * ratio)\n",
    "        max20 = torch.mean(sorted_values[:top10_len])\n",
    "        heatmap = torch.clamp(heatmap / max20, min=0., max=1.)\n",
    "        return heatmap\n",
    "\n",
    "    def refine_heatmap_local(self, heatmap, num_blocks=5, overlap_ratio=0.5,\n",
    "                             ratio=0.2, valid_thresh=2e-3):\n",
    "        \"\"\" Local heatmap refinement method. \"\"\"\n",
    "        # Get the shape of the heatmap\n",
    "        H, W = heatmap.shape\n",
    "        increase_ratio = 1 - overlap_ratio\n",
    "        h_block = round(H / (1 + (num_blocks - 1) * increase_ratio))\n",
    "        w_block = round(W / (1 + (num_blocks - 1) * increase_ratio))\n",
    "\n",
    "        count_map = torch.zeros(heatmap.shape, dtype=torch.float,\n",
    "                                device=heatmap.device)\n",
    "        heatmap_output = torch.zeros(heatmap.shape, dtype=torch.float,\n",
    "                                     device=heatmap.device)\n",
    "        # Iterate through each block\n",
    "        for h_idx in range(num_blocks):\n",
    "            for w_idx in range(num_blocks):\n",
    "                # Fetch the heatmap\n",
    "                h_start = round(h_idx * h_block * increase_ratio)\n",
    "                w_start = round(w_idx * w_block * increase_ratio)\n",
    "                h_end = h_start + h_block if h_idx < num_blocks - 1 else H\n",
    "                w_end = w_start + w_block if w_idx < num_blocks - 1 else W\n",
    "\n",
    "                subheatmap = heatmap[h_start:h_end, w_start:w_end]\n",
    "                if subheatmap.max() > valid_thresh:\n",
    "                    subheatmap = self.refine_heatmap(\n",
    "                        subheatmap, ratio, valid_thresh=valid_thresh)\n",
    "\n",
    "                # Aggregate it to the final heatmap\n",
    "                heatmap_output[h_start:h_end, w_start:w_end] += subheatmap\n",
    "                count_map[h_start:h_end, w_start:w_end] += 1\n",
    "        heatmap_output = torch.clamp(heatmap_output / count_map,\n",
    "                                     max=1., min=0.)\n",
    "\n",
    "        return heatmap_output\n",
    "\n",
    "    def candidate_suppression(self, junctions, candidate_map):\n",
    "        \"\"\" Suppress overlapping long lines in the candidate segments. \"\"\"\n",
    "        # Define the distance tolerance\n",
    "        dist_tolerance = self.nms_dist_tolerance\n",
    "\n",
    "        # Compute distance between junction pairs\n",
    "        # (num_junc x 1 x 2) - (1 x num_junc x 2) => num_junc x num_junc map\n",
    "        line_dist_map = torch.sum((torch.unsqueeze(junctions, dim=1)\n",
    "                                   - junctions[None, ...]) ** 2, dim=-1) ** 0.5\n",
    "\n",
    "        # Fetch all the \"detected lines\"\n",
    "        seg_indexes = torch.where(torch.triu(candidate_map, diagonal=1))\n",
    "        start_point_idxs = seg_indexes[0]\n",
    "        end_point_idxs = seg_indexes[1]\n",
    "        start_points = junctions[start_point_idxs, :]\n",
    "        end_points = junctions[end_point_idxs, :]\n",
    "\n",
    "        # Fetch corresponding entries\n",
    "        line_dists = line_dist_map[start_point_idxs, end_point_idxs]\n",
    "\n",
    "        # Check whether they are on the line\n",
    "        dir_vecs = ((end_points - start_points)\n",
    "                    / torch.norm(end_points - start_points,\n",
    "                                 dim=-1)[..., None])\n",
    "        # Get the orthogonal distance\n",
    "        cand_vecs = junctions[None, ...] - start_points.unsqueeze(dim=1)\n",
    "        cand_vecs_norm = torch.norm(cand_vecs, dim=-1)\n",
    "        # Check whether they are projected directly onto the segment\n",
    "        proj = (torch.einsum('bij,bjk->bik', cand_vecs, dir_vecs[..., None])\n",
    "                / line_dists[..., None, None])\n",
    "        # proj is num_segs x num_junction x 1\n",
    "        proj_mask = (proj >= 0) * (proj <= 1)\n",
    "        cand_angles = torch.acos(\n",
    "            torch.einsum('bij,bjk->bik', cand_vecs, dir_vecs[..., None])\n",
    "            / cand_vecs_norm[..., None])\n",
    "        cand_dists = cand_vecs_norm[..., None] * torch.sin(cand_angles)\n",
    "        junc_dist_mask = cand_dists <= dist_tolerance\n",
    "        junc_mask = junc_dist_mask * proj_mask\n",
    "\n",
    "        # Minus starting points\n",
    "        num_segs = start_point_idxs.shape[0]\n",
    "        junc_counts = torch.sum(junc_mask, dim=[1, 2])\n",
    "        junc_counts -= junc_mask[..., 0][torch.arange(0, num_segs),\n",
    "        start_point_idxs].to(torch.int)\n",
    "        junc_counts -= junc_mask[..., 0][torch.arange(0, num_segs),\n",
    "        end_point_idxs].to(torch.int)\n",
    "\n",
    "        # Get the invalid candidate mask\n",
    "        final_mask = junc_counts > 0\n",
    "        candidate_map[start_point_idxs[final_mask],\n",
    "        end_point_idxs[final_mask]] = 0\n",
    "\n",
    "        return candidate_map\n",
    "\n",
    "    def refine_junction_perturb(self, junctions, line_map_pred,\n",
    "                                heatmap, H, W, device):\n",
    "        \"\"\" Refine the line endpoints in a similar way as in LSD. \"\"\"\n",
    "        # Get the config\n",
    "        junction_refine_cfg = self.junction_refine_cfg\n",
    "\n",
    "        # Fetch refinement parameters\n",
    "        num_perturbs = junction_refine_cfg[\"num_perturbs\"]\n",
    "        perturb_interval = junction_refine_cfg[\"perturb_interval\"]\n",
    "        side_perturbs = (num_perturbs - 1) // 2\n",
    "        # Fetch the 2D perturb mat\n",
    "        perturb_vec = torch.arange(\n",
    "            start=-perturb_interval * side_perturbs,\n",
    "            end=perturb_interval * (side_perturbs + 1),\n",
    "            step=perturb_interval, device=device)\n",
    "        w1_grid, h1_grid, w2_grid, h2_grid = torch.meshgrid(\n",
    "            perturb_vec, perturb_vec, perturb_vec, perturb_vec)\n",
    "        perturb_tensor = torch.cat([\n",
    "            w1_grid[..., None], h1_grid[..., None],\n",
    "            w2_grid[..., None], h2_grid[..., None]], dim=-1)\n",
    "        perturb_tensor_flat = perturb_tensor.view(-1, 2, 2)\n",
    "\n",
    "        # Fetch the junctions and line_map\n",
    "        junctions = junctions.clone()\n",
    "        line_map = line_map_pred\n",
    "\n",
    "        # Fetch all the detected lines\n",
    "        detected_seg_indexes = torch.where(torch.triu(line_map, diagonal=1))\n",
    "        start_point_idxs = detected_seg_indexes[0]\n",
    "        end_point_idxs = detected_seg_indexes[1]\n",
    "        start_points = junctions[start_point_idxs, :]\n",
    "        end_points = junctions[end_point_idxs, :]\n",
    "\n",
    "        line_segments = torch.cat([start_points.unsqueeze(dim=1),\n",
    "                                   end_points.unsqueeze(dim=1)], dim=1)\n",
    "\n",
    "        line_segment_candidates = (line_segments.unsqueeze(dim=1)\n",
    "                                   + perturb_tensor_flat[None, ...])\n",
    "        # Clip the boundaries\n",
    "        line_segment_candidates[..., 0] = torch.clamp(\n",
    "            line_segment_candidates[..., 0], min=0, max=H - 1)\n",
    "        line_segment_candidates[..., 1] = torch.clamp(\n",
    "            line_segment_candidates[..., 1], min=0, max=W - 1)\n",
    "\n",
    "        # Iterate through all the segments\n",
    "        refined_segment_lst = []\n",
    "        num_segments = line_segments.shape[0]\n",
    "        for idx in range(num_segments):\n",
    "            segment = line_segment_candidates[idx, ...]\n",
    "            # Get the corresponding start and end junctions\n",
    "            candidate_junc_start = segment[:, 0, :]\n",
    "            candidate_junc_end = segment[:, 1, :]\n",
    "\n",
    "            # Get the sampling locations (N x 64)\n",
    "            sampler = self.torch_sampler.to(device)[None, ...]\n",
    "            cand_samples_h = (candidate_junc_start[:, 0:1] * sampler +\n",
    "                              candidate_junc_end[:, 0:1] * (1 - sampler))\n",
    "            cand_samples_w = (candidate_junc_start[:, 1:2] * sampler +\n",
    "                              candidate_junc_end[:, 1:2] * (1 - sampler))\n",
    "\n",
    "            # Clip to image boundary\n",
    "            cand_h = torch.clamp(cand_samples_h, min=0, max=H - 1)\n",
    "            cand_w = torch.clamp(cand_samples_w, min=0, max=W - 1)\n",
    "\n",
    "            # Perform bilinear sampling\n",
    "            segment_feat = self.detect_bilinear(\n",
    "                heatmap, cand_h, cand_w, H, W, device)\n",
    "            segment_results = torch.mean(segment_feat, dim=-1)\n",
    "            max_idx = torch.argmax(segment_results)\n",
    "            refined_segment_lst.append(segment[max_idx, ...][None, ...])\n",
    "\n",
    "        # Concatenate back to segments\n",
    "        refined_segments = torch.cat(refined_segment_lst, dim=0)\n",
    "\n",
    "        # Convert back to junctions and line_map\n",
    "        junctions_new = torch.cat(\n",
    "            [refined_segments[:, 0, :], refined_segments[:, 1, :]], dim=0)\n",
    "        junctions_new = torch.unique(junctions_new, dim=0)\n",
    "        line_map_new = self.segments_to_line_map(junctions_new,\n",
    "                                                 refined_segments)\n",
    "\n",
    "        return junctions_new, line_map_new\n",
    "\n",
    "    def segments_to_line_map(self, junctions, segments):\n",
    "        \"\"\" Convert the list of segments to line map. \"\"\"\n",
    "        # Create empty line map\n",
    "        device = junctions.device\n",
    "        num_junctions = junctions.shape[0]\n",
    "        line_map = torch.zeros([num_junctions, num_junctions], device=device)\n",
    "\n",
    "        # Iterate through every segment\n",
    "        for idx in range(segments.shape[0]):\n",
    "            # Get the junctions from a single segement\n",
    "            seg = segments[idx, ...]\n",
    "            junction1 = seg[0, :]\n",
    "            junction2 = seg[1, :]\n",
    "\n",
    "            # Get index\n",
    "            idx_junction1 = torch.where(\n",
    "                (junctions == junction1).sum(axis=1) == 2)[0]\n",
    "            idx_junction2 = torch.where(\n",
    "                (junctions == junction2).sum(axis=1) == 2)[0]\n",
    "\n",
    "            # label the corresponding entries\n",
    "            line_map[idx_junction1, idx_junction2] = 1\n",
    "            line_map[idx_junction2, idx_junction1] = 1\n",
    "\n",
    "        return line_map\n",
    "\n",
    "    def detect_bilinear(self, heatmap, cand_h, cand_w, H, W, device):\n",
    "        \"\"\" Detection by bilinear sampling. \"\"\"\n",
    "        # Get the floor and ceiling locations\n",
    "        cand_h_floor = torch.floor(cand_h).to(torch.long)\n",
    "        cand_h_ceil = torch.ceil(cand_h).to(torch.long)\n",
    "        cand_w_floor = torch.floor(cand_w).to(torch.long)\n",
    "        cand_w_ceil = torch.ceil(cand_w).to(torch.long)\n",
    "\n",
    "        # Perform the bilinear sampling\n",
    "        cand_samples_feat = (\n",
    "                heatmap[cand_h_floor, cand_w_floor] * (cand_h_ceil - cand_h)\n",
    "                * (cand_w_ceil - cand_w) + heatmap[cand_h_floor, cand_w_ceil]\n",
    "                * (cand_h_ceil - cand_h) * (cand_w - cand_w_floor) +\n",
    "                heatmap[cand_h_ceil, cand_w_floor] * (cand_h - cand_h_floor)\n",
    "                * (cand_w_ceil - cand_w) + heatmap[cand_h_ceil, cand_w_ceil]\n",
    "                * (cand_h - cand_h_floor) * (cand_w - cand_w_floor))\n",
    "\n",
    "        return cand_samples_feat\n",
    "\n",
    "    def detect_local_max(self, heatmap, cand_h, cand_w, H, W,\n",
    "                         normalized_seg_length, device):\n",
    "        \"\"\" Detection by local maximum search. \"\"\"\n",
    "        # Compute the distance threshold\n",
    "        dist_thresh = (0.5 * (2 ** 0.5)\n",
    "                       + self.lambda_radius * normalized_seg_length)\n",
    "        # Make it N x 64\n",
    "        dist_thresh = torch.repeat_interleave(dist_thresh[..., None],\n",
    "                                              self.num_samples, dim=-1)\n",
    "\n",
    "        # Compute the candidate points\n",
    "        cand_points = torch.cat([cand_h[..., None], cand_w[..., None]],\n",
    "                                dim=-1)\n",
    "        cand_points_round = torch.round(cand_points)  # N x 64 x 2\n",
    "\n",
    "        # Construct local patches 9x9 = 81\n",
    "        patch_mask = torch.zeros([int(2 * self.local_patch_radius + 1),\n",
    "                                  int(2 * self.local_patch_radius + 1)],\n",
    "                                 device=device)\n",
    "        patch_center = torch.tensor(\n",
    "            [[self.local_patch_radius, self.local_patch_radius]],\n",
    "            device=device, dtype=torch.float32)\n",
    "        H_patch_points, W_patch_points = torch.where(patch_mask >= 0)\n",
    "        patch_points = torch.cat([H_patch_points[..., None],\n",
    "                                  W_patch_points[..., None]], dim=-1)\n",
    "        # Fetch the circle region\n",
    "        patch_center_dist = torch.sqrt(torch.sum(\n",
    "            (patch_points - patch_center) ** 2, dim=-1))\n",
    "        patch_points = (patch_points[patch_center_dist\n",
    "                                     <= self.local_patch_radius, :])\n",
    "        # Shift [0, 0] to the center\n",
    "        patch_points = patch_points - self.local_patch_radius\n",
    "\n",
    "        # Construct local patch mask\n",
    "        patch_points_shifted = (torch.unsqueeze(cand_points_round, dim=2)\n",
    "                                + patch_points[None, None, ...])\n",
    "        patch_dist = torch.sqrt(torch.sum((torch.unsqueeze(cand_points, dim=2)\n",
    "                                           - patch_points_shifted) ** 2,\n",
    "                                          dim=-1))\n",
    "        patch_dist_mask = patch_dist < dist_thresh[..., None]\n",
    "\n",
    "        # Get all points => num_points_center x num_patch_points x 2\n",
    "        points_H = torch.clamp(patch_points_shifted[:, :, :, 0], min=0,\n",
    "                               max=H - 1).to(torch.long)\n",
    "        points_W = torch.clamp(patch_points_shifted[:, :, :, 1], min=0,\n",
    "                               max=W - 1).to(torch.long)\n",
    "        points = torch.cat([points_H[..., None], points_W[..., None]], dim=-1)\n",
    "\n",
    "        # Sample the feature (N x 64 x 81)\n",
    "        sampled_feat = heatmap[points[:, :, :, 0], points[:, :, :, 1]]\n",
    "        # Filtering using the valid mask\n",
    "        sampled_feat = sampled_feat * patch_dist_mask.to(torch.float32)\n",
    "        if len(sampled_feat) == 0:\n",
    "            sampled_feat_lmax = torch.empty(0, 64)\n",
    "        else:\n",
    "            sampled_feat_lmax, _ = torch.max(sampled_feat, dim=-1)\n",
    "\n",
    "        return sampled_feat_lmax\n"
   ],
   "id": "d3ba37435c89de15",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Other utils",
   "id": "9fd1714a0df7b705"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:31:44.498942Z",
     "start_time": "2024-04-15T16:31:44.492410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InputPadder(object):\n",
    "    \"\"\"Pads images such that dimensions are divisible by 8\"\"\"\n",
    "\n",
    "    def __init__(self, h: int, w: int, divis_by: int = 8):\n",
    "        self.ht = h\n",
    "        self.wd = w\n",
    "        pad_ht = (((self.ht // divis_by) + 1) * divis_by - self.ht) % divis_by\n",
    "        pad_wd = (((self.wd // divis_by) + 1) * divis_by - self.wd) % divis_by\n",
    "        self._pad = [\n",
    "            pad_wd // 2,\n",
    "            pad_wd - pad_wd // 2,\n",
    "            pad_ht // 2,\n",
    "            pad_ht - pad_ht // 2,\n",
    "        ]\n",
    "\n",
    "    def pad(self, x: torch.Tensor):\n",
    "        assert x.ndim == 4\n",
    "        return F.pad(x, self._pad, mode=\"replicate\")\n",
    "\n",
    "    def unpad(self, x: torch.Tensor):\n",
    "        assert x.ndim == 4\n",
    "        ht = x.shape[-2]\n",
    "        wd = x.shape[-1]\n",
    "        c = [self._pad[2], ht - self._pad[3], self._pad[0], wd - self._pad[1]]\n",
    "        return x[..., c[0]: c[1], c[2]: c[3]]"
   ],
   "id": "58574bc2cdbbd761",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Basemodel",
   "id": "e853d41e4939635"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:31:47.009559Z",
     "start_time": "2024-04-15T16:31:46.869903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from copy import copy\n",
    "\n",
    "import omegaconf\n",
    "from omegaconf import OmegaConf\n",
    "from torch import nn\n",
    "\n",
    "class MetaModel(ABCMeta):\n",
    "    def __prepare__(name, bases, **kwds):\n",
    "        total_conf = OmegaConf.create()\n",
    "        for base in bases:\n",
    "            for key in (\"base_default_conf\", \"default_conf\"):\n",
    "                update = getattr(base, key, {})\n",
    "                if isinstance(update, dict):\n",
    "                    update = OmegaConf.create(update)\n",
    "                total_conf = OmegaConf.merge(total_conf, update)\n",
    "        return dict(base_default_conf=total_conf)\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module, metaclass=MetaModel):\n",
    "    \"\"\"\n",
    "    What the child model is expect to declare:\n",
    "        default_conf: dictionary of the default configuration of the model.\n",
    "        It recursively updates the default_conf of all parent classes, and\n",
    "        it is updated by the user-provided configuration passed to __init__.\n",
    "        Configurations can be nested.\n",
    "\n",
    "        required_data_keys: list of expected keys in the input data dictionary.\n",
    "\n",
    "        strict_conf (optional): boolean. If false, BaseModel does not raise\n",
    "        an error when the user provides an unknown configuration entry.\n",
    "\n",
    "        _init(self, conf): initialization method, where conf is the final\n",
    "        configuration object (also accessible with `self.conf`). Accessing\n",
    "        unknown configuration entries will raise an error.\n",
    "\n",
    "        _forward(self, data): method that returns a dictionary of batched\n",
    "        prediction tensors based on a dictionary of batched input data tensors.\n",
    "\n",
    "        loss(self, pred, data): method that returns a dictionary of losses,\n",
    "        computed from model predictions and input data. Each loss is a batch\n",
    "        of scalars, i.e. a torch.Tensor of shape (B,).\n",
    "        The total loss to be optimized has the key `'total'`.\n",
    "\n",
    "        metrics(self, pred, data): method that returns a dictionary of metrics,\n",
    "        each as a batch of scalars.\n",
    "    \"\"\"\n",
    "\n",
    "    default_conf = {\n",
    "        \"name\": None,\n",
    "        \"trainable\": True,  # if false: do not optimize this model parameters\n",
    "        \"freeze_batch_normalization\": False,  # use test-time statistics\n",
    "        \"timeit\": False,  # time forward pass\n",
    "    }\n",
    "    required_data_keys = []\n",
    "    strict_conf = False\n",
    "\n",
    "    are_weights_initialized = False\n",
    "\n",
    "    def __init__(self, conf):\n",
    "        \"\"\"Perform some logic and call the _init method of the child model.\"\"\"\n",
    "        super().__init__()\n",
    "        default_conf = OmegaConf.merge(\n",
    "            self.base_default_conf, OmegaConf.create(self.default_conf)\n",
    "        )\n",
    "        if self.strict_conf:\n",
    "            OmegaConf.set_struct(default_conf, True)\n",
    "\n",
    "        # fixme: backward compatibility\n",
    "        if \"pad\" in conf and \"pad\" not in default_conf:  # backward compat.\n",
    "            with omegaconf.read_write(conf):\n",
    "                with omegaconf.open_dict(conf):\n",
    "                    conf[\"interpolation\"] = {\"pad\": conf.pop(\"pad\")}\n",
    "\n",
    "        if isinstance(conf, dict):\n",
    "            conf = OmegaConf.create(conf)\n",
    "        self.conf = conf = OmegaConf.merge(default_conf, conf)\n",
    "        OmegaConf.set_readonly(conf, True)\n",
    "        OmegaConf.set_struct(conf, True)\n",
    "        self.required_data_keys = copy(self.required_data_keys)\n",
    "        self._init(conf)\n",
    "\n",
    "        if not conf.trainable:\n",
    "            for p in self.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "\n",
    "        def freeze_bn(module):\n",
    "            if isinstance(module, nn.modules.batchnorm._BatchNorm):\n",
    "                module.eval()\n",
    "\n",
    "        if self.conf.freeze_batch_normalization:\n",
    "            self.apply(freeze_bn)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"Check the data and call the _forward method of the child model.\"\"\"\n",
    "\n",
    "        def recursive_key_check(expected, given):\n",
    "            for key in expected:\n",
    "                assert key in given, f\"Missing key {key} in data\"\n",
    "                if isinstance(expected, dict):\n",
    "                    recursive_key_check(expected[key], given[key])\n",
    "\n",
    "        recursive_key_check(self.required_data_keys, data)\n",
    "        return self._forward(data)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _init(self, conf):\n",
    "        \"\"\"To be implemented by the child class.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def _forward(self, data):\n",
    "        \"\"\"To be implemented by the child class.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss(self, pred, data):\n",
    "        \"\"\"To be implemented by the child class.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_state_dict(self, *args, **kwargs):\n",
    "        \"\"\"Load the state dict of the model, and set the model to initialized.\"\"\"\n",
    "        ret = super().load_state_dict(*args, **kwargs)\n",
    "        self.set_initialized()\n",
    "        return ret\n",
    "\n",
    "    def is_initialized(self):\n",
    "        \"\"\"Recursively check if the model is initialized, i.e. weights are loaded\"\"\"\n",
    "        is_initialized = True  # initialize to true and perform recursive and\n",
    "        for _, w in self.named_children():\n",
    "            if isinstance(w, BaseModel):\n",
    "                # if children is BaseModel, we perform recursive check\n",
    "                is_initialized = is_initialized and w.is_initialized()\n",
    "            else:\n",
    "                # else, we check if self is initialized or the children has no params\n",
    "                n_params = len(list(w.parameters()))\n",
    "                is_initialized = is_initialized and (\n",
    "                    n_params == 0 or self.are_weights_initialized\n",
    "                )\n",
    "        return is_initialized\n",
    "\n",
    "    def set_initialized(self, to: bool = True):\n",
    "        \"\"\"Recursively set the initialization state.\"\"\"\n",
    "        self.are_weights_initialized = to\n",
    "        for _, w in self.named_parameters():\n",
    "            if isinstance(w, BaseModel):\n",
    "                w.set_initialized(to)"
   ],
   "id": "469ec5672287e8fd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Network file",
   "id": "3521bc808a572068"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:57:06.456517Z",
     "start_time": "2024-04-15T16:57:06.443310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class JointPointLineDetectorDescriptor(BaseModel):\n",
    "    # currently contains only ALIKED\n",
    "    default_conf = {  # ToDo: create default conf once everything is running -> default conf is merged with input conf to the init method!\n",
    "        \"model_name\": \"aliked-n16\",\n",
    "        \"max_num_keypoints\": -1,\n",
    "        \"detection_threshold\": 0.2,\n",
    "        \"force_num_keypoints\": False,\n",
    "        \"pretrained\": True,\n",
    "        \"nms_radius\": 2,\n",
    "    }\n",
    "    \n",
    "    n_limit_max = 20000 # taken from ALIKED which gives max num keypoints to detect! ToDo\n",
    "    \n",
    "    line_extractor_cfg = {\n",
    "        \"detect_thresh\": 1/65,\n",
    "        \"grid_size\": 8,\n",
    "        \"junc_detect_thresh\": 1/65,\n",
    "        \"max_num_junctions\": 300\n",
    "    }\n",
    "\n",
    "    required_data_keys = [\"image\"]\n",
    "\n",
    "    def _init(self, conf):\n",
    "        print(f\"final config dict(type={type(conf)}): {conf}\")\n",
    "        # get configurations\n",
    "        # c1-c4 -> output dimensions of encoder blocks, dim -> dimension of hidden feature map\n",
    "        # K=Kernel-Size, M=num sampling pos\n",
    "        aliked_model_cfg = aliked_cfgs[conf.model_name]\n",
    "        dim = aliked_model_cfg[\"dim\"]\n",
    "        K = aliked_model_cfg[\"K\"]\n",
    "        M = aliked_model_cfg[\"M\"]\n",
    "        # Load Network Components\n",
    "        print(f\"aliked cfg(type={type(aliked_model_cfg)}): {aliked_model_cfg}\")\n",
    "        self.encoder = AlikedEncoder(aliked_model_cfg)\n",
    "        self.keypoint_and_junction_branch = SMH(dim)  # using SMH from ALIKE here\n",
    "        self.dkd = DKD(radius=conf.nms_radius,\n",
    "                       top_k=-1 if conf.detection_threshold > 0 else conf.max_num_keypoints,\n",
    "                       scores_th=conf.detection_threshold,\n",
    "                       n_limit=(\n",
    "                           conf.max_num_keypoints\n",
    "                           if conf.max_num_keypoints > 0\n",
    "                           else self.n_limit_max\n",
    "                       ), )  # Differentiable Keypoint Detection from ALIKE\n",
    "        self.descriptor_branch = SDDH(dim, K, M, gate=nn.SELU(inplace=True), conv2D=False, mask=False)\n",
    "        self.line_heatmap_branch = PixelShuffleDecoder(input_feat_dim=dim)  # Use SOLD2 branch\n",
    "        self.line_extractor = LineExtractor(torch.device(\"cpu\"), self.line_extractor_cfg)  # USe SOLD2 one\n",
    "        self.line_descriptor = torch.lerp  # we take the endpoints of lines and interpolate to get the descriptor\n",
    "\n",
    "    def _forward(self, data):\n",
    "        # load image and padder\n",
    "        image = data[\"image\"]\n",
    "        div_by = 2 ** 5\n",
    "        print(f\"image({type(image)}): {image.shape}\")\n",
    "        padder = InputPadder(image.shape[-2], image.shape[-1], div_by)\n",
    "\n",
    "        # Get Hidden Feature Map and Keypoint/junction scoring\n",
    "        feature_map_padded = self.encoder(padder.pad(image))\n",
    "        score_map_padded = self.keypoint_and_junction_branch(feature_map_padded)\n",
    "        feature_map_padded_normalized = torch.nn.functional.normalize(feature_map_padded, p=2, dim=1)\n",
    "        feature_map = padder.unpad(feature_map_padded_normalized)\n",
    "        keypoint_and_junction_score_map = padder.unpad(score_map_padded)\n",
    "\n",
    "        line_heatmap = self.line_heatmap_branch.forward(feature_map)\n",
    "\n",
    "        keypoints, kptscores, scoredispersitys = self.dkd(\n",
    "            keypoint_and_junction_score_map, image_size=data.get(\"image_size\")\n",
    "        )\n",
    "\n",
    "        # ToDo: Does it work well to use keypoints for juctions?? -> Design decision\n",
    "        # ToDo: need preprocessing like in SOLD2 repo before passing it to line extractor?\n",
    "        line_map, junctions, heatmap = self.line_extractor(keypoints, line_heatmap)\n",
    "        line_segments = line_map_to_segments(junctions, line_map)\n",
    "\n",
    "        descriptors, offsets = self.desc_head(feature_map, keypoints)\n",
    "        # TODO: can we make sure endpoints are always keypoints?! + Fix Input to this function\n",
    "        line_descriptors = self.line_descriptor(line_segments[0], line_segments[1], 0.5)  # TODO: Interpolate line-endpoint descriptors\n",
    "\n",
    "        _, _, h, w = image.shape\n",
    "        wh = torch.tensor([w, h], device=image.device)\n",
    "        # no padding required,\n",
    "        # we can set detection_threshold=-1 and conf.max_num_keypoints\n",
    "        return {\n",
    "            \"keypoints\": wh * (torch.stack(keypoints) + 1) / 2.0,  # B N 2\n",
    "            \"keypoint_descriptors\": torch.stack(descriptors),  # B N D\n",
    "            \"keypoint_scores\": torch.stack(kptscores),  # B N\n",
    "            \"score_dispersity\": torch.stack(scoredispersitys),\n",
    "            \"score_map\": keypoint_and_junction_score_map,  # Bx1xHxW\n",
    "            \"line_heatmap\": heatmap,\n",
    "            \"line_endpoints\": line_segments,  # as tuples\n",
    "            \"line_descriptors\": line_descriptors  # as vectors\n",
    "        }\n",
    "\n",
    "    def loss(self, pred, data):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def count_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ],
   "id": "b8940d2082da0715",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:31:53.479687Z",
     "start_time": "2024-04-15T16:31:53.474540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ToDo: Figure out default config\n",
    "config = {\n",
    "    \"backbone-encoder\": {\n",
    "        \"name\": \"ALIKED\" # name to (load?)/name backbone encoder rest of config is defined by default conf or passed here -> default conf is taken and overwritten by passed conf if/where given\n",
    "    },\n",
    "    \"keypoint-and-junction-decoder\": {\n",
    "        \"name\": \"ALIKED-SMH\" # name to (load?)/name backbone encoder rest of config is defined by default conf or passed here -> default conf is taken and overwritten by passed conf if/where given\n",
    "    },\n",
    "    \"keypoint-detector\": {\n",
    "        \"name\": \"ALIKE-DKD\" # name to (load?)/name backbone encoder rest of config is defined by default conf or passed here -> default conf is taken and overwritten by passed conf if/where given\n",
    "    },\n",
    "    \"line-heatmap-decoder\": {\n",
    "        \"name\": \"SOLD2-PixelShuffle\" # name to (load?)/name backbone encoder rest of config is defined by default conf or passed here -> default conf is taken and overwritten by passed conf if/where given\n",
    "    },\n",
    "    \"line-detector\": {\n",
    "        \"name\": \"SOLD2-Lineextractor\" # name to (load?)/name backbone encoder rest of config is defined by default conf or passed here -> default conf is taken and overwritten by passed conf if/where given\n",
    "    },\n",
    "    \"descriptors\": {\n",
    "        \"name\": \"ALIKED-SDDH\"  # name to (load?)/name backbone encoder rest of config is defined by default conf or passed here -> default conf is taken and overwritten by passed conf if/where given\n",
    "    }\n",
    "}"
   ],
   "id": "842661ff2f0886f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing",
   "id": "25010a8ba27270d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T16:46:35.444770Z",
     "start_time": "2024-04-15T16:46:35.438933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# taken from glue_factory.utils.image\n",
    "def read_image(path, grayscale=False):\n",
    "    \"\"\"Read an image from path as RGB or grayscale\"\"\"\n",
    "    mode = cv2.IMREAD_GRAYSCALE if grayscale else cv2.IMREAD_COLOR\n",
    "    image = cv2.imread(str(path), mode)\n",
    "    if image is None:\n",
    "        raise IOError(f\"Could not read image at {path}.\")\n",
    "    if not grayscale:\n",
    "        image = image[..., ::-1]\n",
    "    return image\n",
    "\n",
    "from PIL import Image # alternative to load image\n",
    "\n",
    "def load_img_pil(path, resize=True, as_npy=True):\n",
    "    img = Image.open(path)\n",
    "    if resize:\n",
    "        img = torchvision.transforms.Resize(800)(img)\n",
    "    if as_npy:\n",
    "        img_npy = torchvision.transforms.ToTensor()(img).numpy()\n",
    "        img_npy = np.expand_dims(img_npy, axis=0).copy() # add artificial batch dimension\n",
    "        #img_npy = np.transpose(img_npy, (0, 3, 1, 2)) # reshape\n",
    "        return img_npy\n",
    "    return img\n",
    "# print(img.shape)\n",
    "# plt.imshow(img)"
   ],
   "id": "b07b2fff6ce3598f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c4d646c1544f840e"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-15T16:57:58.122826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load img\n",
    "img_path = \"sample_eiffel.jpg\"\n",
    "img = load_img_pil(img_path, resize=True, as_npy=True) # load, rescale and reshape \n",
    "# initialize\n",
    "device = torch.device(\"cpu\")\n",
    "our_network = JointPointLineDetectorDescriptor(config)\n",
    "num_params = our_network.count_trainable_parameters()\n",
    "print(f\"Num-Parameters trainable: {num_params}\")\n",
    "our_network.to(device)\n",
    "our_network.eval()\n",
    "\n",
    "# Prepare Img\n",
    " # rescale to 800 x 800\n",
    "print(f\"img shape: {img.shape}\")\n",
    "img = torch.from_numpy(img).float().to(device) # convert to tensor and move onto device\n",
    "\n",
    "# run eval\n",
    "with torch.no_grad():\n",
    "    test_data = {\"image\": img}\n",
    "    predictions = our_network.forward(test_data)\n",
    "predictions"
   ],
   "id": "64524d3359dfbb72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final config dict(type=<class 'omegaconf.dictconfig.DictConfig'>): {'name': None, 'trainable': True, 'freeze_batch_normalization': False, 'timeit': False, 'model_name': 'aliked-n16', 'max_num_keypoints': -1, 'detection_threshold': 0.2, 'force_num_keypoints': False, 'pretrained': True, 'nms_radius': 2, 'backbone-encoder': {'name': 'ALIKED'}, 'keypoint-and-junction-decoder': {'name': 'ALIKED-SMH'}, 'keypoint-detector': {'name': 'ALIKE-DKD'}, 'line-heatmap-decoder': {'name': 'SOLD2-PixelShuffle'}, 'line-detector': {'name': 'SOLD2-Lineextractor'}, 'descriptors': {'name': 'ALIKED-SDDH'}}\n",
      "aliked cfg(type=<class 'dict'>): {'c1': 16, 'c2': 32, 'c3': 64, 'c4': 128, 'dim': 128, 'K': 3, 'M': 16}\n",
      "Num-Parameters trainable: 1010126\n",
      "img shape: (1, 3, 1332, 800)\n",
      "torch.float32\n",
      "image(<class 'torch.Tensor'>): torch.Size([1, 3, 1332, 800])\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7cc0b582ef8df885"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
